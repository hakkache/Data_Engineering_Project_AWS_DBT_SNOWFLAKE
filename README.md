# ğŸš€ Brazilian E-Commerce Data Engineering Project

[![DBT](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)](https://www.getdbt.com/)
[![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?style=for-the-badge&logo=snowflake&logoColor=white)](https://www.snowflake.com/)
[![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io/)

A comprehensive end-to-end data engineering pipeline built on modern cloud infrastructure, featuring data ingestion from AWS S3, transformation using dbt, warehousing in Snowflake, machine learning predictions, and interactive analytics dashboards.

## ğŸ“š Interactive Documentation

**[ğŸŒ View Interactive Documentation](https://hakkache.github.io/Data_Engineering_Project_AWS_DBT_SNOWFLAKE/)** - Explore the complete architecture, data flow, and system components through our interactive documentation site.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Data Flow](#data-flow)
- [Project Structure](#project-structure)
- [Technology Stack](#technology-stack)
- [Features](#features)
- [Setup & Installation](#setup--installation)
- [Usage](#usage)
- [Data Models](#data-models)
- [Dashboard](#dashboard)
- [Machine Learning Pipeline](#machine-learning-pipeline)
- [Contributing](#contributing)

## ğŸ¯ Overview

This project demonstrates a production-ready data engineering solution for analyzing **[Brazilian E-Commerce Public Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)** (2016-2018) from Kaggle/Olist. The pipeline ingests raw data from AWS S3, transforms it through bronze/silver/gold layers using dbt, stores it in Snowflake, runs ML models for predictions, and visualizes insights through an interactive Streamlit dashboard.

**Key Highlights:**
- âœ… **100K+ orders** processed across 3 years
- âœ… **8 source tables** integrated from Brazilian e-commerce marketplace
- âœ… **3-tier medallion architecture** (Bronze â†’ Silver â†’ Gold)
- âœ… **20+ dbt models** with data quality tests
- âœ… **Interactive dashboard** with 7 analytics pages
- âœ… **ML pipeline** for customer behavior prediction

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    subgraph "Data Sources"
        S3[("AWS S3<br/>Raw CSV Files")]
    end
    
    subgraph "Data Warehouse"
        SF[(Snowflake)]
        RAW[Raw Schema]
        BRONZE[Bronze Schema<br/>Staging Layer]
        SILVER[Silver Schema<br/>Cleaned & Conformed]
        GOLD[Gold Schema<br/>Business Layer]
    end
    
    subgraph "Transformation Layer"
        DBT[dbt Core<br/>Data Transformations]
        TESTS[Data Quality Tests]
        DOCS[Documentation]
    end
    
    subgraph "ML Pipeline"
        FEATURE[Feature Engineering]
        TRAIN[Model Training<br/>CatBoost]
        PREDICT[Predictions]
    end
    
    subgraph "Analytics Layer"
        DASH[Streamlit Dashboard<br/>7 Pages]
        VIZ[Interactive<br/>Visualizations]
    end
    
    subgraph "Users"
        BA[Business Analysts]
        DS[Data Scientists]
        EXE[Executives]
    end
    
    S3 -->|COPY INTO| RAW
    RAW -->|dbt run| BRONZE
    BRONZE -->|dbt run| SILVER
    SILVER -->|dbt run| GOLD
    
    DBT -->|Transform| BRONZE
    DBT -->|Transform| SILVER
    DBT -->|Transform| GOLD
    
    TESTS -.->|Validate| DBT
    DBT -.->|Generate| DOCS
    
    GOLD -->|Query| FEATURE
    FEATURE -->|Train| TRAIN
    TRAIN -->|Deploy| PREDICT
    PREDICT -->|Store| GOLD
    
    GOLD -->|Query| DASH
    DASH -->|Display| VIZ
    
    VIZ -->|Insights| BA
    VIZ -->|Insights| DS
    VIZ -->|Insights| EXE
    
    style S3 fill:#FF9900,stroke:#CC7A00,stroke-width:3px,color:#000
    style SF fill:#29B5E8,stroke:#1E8AC4,stroke-width:3px,color:#FFF
    style RAW fill:#E8E8E8,stroke:#999,stroke-width:2px,color:#000
    style BRONZE fill:#CD7F32,stroke:#8B5A00,stroke-width:3px,color:#FFF
    style SILVER fill:#C0C0C0,stroke:#808080,stroke-width:3px,color:#000
    style GOLD fill:#FFD700,stroke:#DAA520,stroke-width:3px,color:#000
    style DBT fill:#FF694B,stroke:#CC4428,stroke-width:3px,color:#FFF
    style TESTS fill:#FFA07A,stroke:#E67451,stroke-width:2px,color:#000
    style DOCS fill:#FFB6C1,stroke:#FF8FAB,stroke-width:2px,color:#000
    style FEATURE fill:#90EE90,stroke:#32CD32,stroke-width:2px,color:#000
    style TRAIN fill:#4CAF50,stroke:#388E3C,stroke-width:3px,color:#FFF
    style PREDICT fill:#66BB6A,stroke:#43A047,stroke-width:2px,color:#FFF
    style DASH fill:#FF4B4B,stroke:#CC3232,stroke-width:3px,color:#FFF
    style VIZ fill:#FF6B6B,stroke:#E64545,stroke-width:2px,color:#FFF
    style BA fill:#64B5F6,stroke:#42A5F5,stroke-width:2px,color:#000
    style DS fill:#81C784,stroke:#66BB6A,stroke-width:2px,color:#000
    style EXE fill:#FFD54F,stroke:#FFCA28,stroke-width:2px,color:#000
```

## ğŸ”„ Data Flow

```mermaid
flowchart LR
    subgraph "Stage 1: Ingestion"
        A1[Raw CSV Files] --> A2[AWS S3 Bucket]
        A2 --> A3[Snowflake RAW Schema<br/>COPY INTO]
    end
    
    subgraph "Stage 2: Bronze Layer"
        A3 --> B1[bronze_olist_orders]
        A3 --> B2[bronze_olist_customers]
        A3 --> B3[bronze_olist_products]
        A3 --> B4[bronze_olist_items]
        A3 --> B5[bronze_olist_reviews]
        A3 --> B6[bronze_olist_payments]
        A3 --> B7[bronze_olist_sellers]
        A3 --> B8[bronze_olist_geolocation]
    end
    
    subgraph "Stage 3: Silver Layer"
        B1 & B2 & B3 & B4 --> C1[silver_orders_enriched]
        B5 --> C2[silver_reviews_cleaned]
        B6 --> C3[silver_payments_agg]
        B7 --> C4[silver_sellers_enhanced]
        C1 & C2 & C3 --> C5[silver_fact_orders]
    end
    
    subgraph "Stage 4: Gold Layer"
        C5 --> D1[gold_fact_order_summary<br/>Aggregated Metrics]
        C1 & C2 & C3 --> D2[gold_obt_orders<br/>One Big Table]
        D2 --> D3[fs_customer_features<br/>Customer Analytics]
    end
    
    subgraph "Stage 5: Consumption"
        D1 & D2 & D3 --> E1[Streamlit Dashboard]
        D3 --> E2[ML Models<br/>CatBoost Predictions]
        E1 --> E3[Business Insights]
        E2 --> E4[Customer Segmentation]
    end
    
    style A1 fill:#E8F5E9,stroke:#4CAF50,stroke-width:2px,color:#000
    style A2 fill:#FF9900,stroke:#CC7A00,stroke-width:3px,color:#FFF
    style A3 fill:#E3F2FD,stroke:#2196F3,stroke-width:2px,color:#000
    
    style B1 fill:#D4845F,stroke:#8B5A00,stroke-width:2px,color:#FFF
    style B2 fill:#D4845F,stroke:#8B5A00,stroke-width:2px,color:#FFF
    style B3 fill:#D4845F,stroke:#8B5A00,stroke-width:2px,color:#FFF
    style B4 fill:#D4845F,stroke:#8B5A00,stroke-width:2px,color:#FFF
    style B5 fill:#D4845F,stroke:#8B5A00,stroke-width:2px,color:#FFF
    style B6 fill:#D4845F,stroke:#8B5A00,stroke-width:2px,color:#FFF
    style B7 fill:#D4845F,stroke:#8B5A00,stroke-width:2px,color:#FFF
    style B8 fill:#D4845F,stroke:#8B5A00,stroke-width:2px,color:#FFF
    
    style C1 fill:#B8B8B8,stroke:#757575,stroke-width:2px,color:#000
    style C2 fill:#B8B8B8,stroke:#757575,stroke-width:2px,color:#000
    style C3 fill:#B8B8B8,stroke:#757575,stroke-width:2px,color:#000
    style C4 fill:#B8B8B8,stroke:#757575,stroke-width:2px,color:#000
    style C5 fill:#9E9E9E,stroke:#616161,stroke-width:3px,color:#FFF
    
    style D1 fill:#FDD835,stroke:#F9A825,stroke-width:3px,color:#000
    style D2 fill:#FFEB3B,stroke:#FBC02D,stroke-width:3px,color:#000
    style D3 fill:#FFEE58,stroke:#FDD835,stroke-width:3px,color:#000
    
    style E1 fill:#FF4B4B,stroke:#D32F2F,stroke-width:3px,color:#FFF
    style E2 fill:#4CAF50,stroke:#388E3C,stroke-width:3px,color:#FFF
    style E3 fill:#64B5F6,stroke:#1976D2,stroke-width:2px,color:#000
    style E4 fill:#81C784,stroke:#66BB6A,stroke-width:2px,color:#000
```

## ğŸ“ Project Structure

```
dbt_snowflake/
â”œâ”€â”€ aws_dbt_snowflake_project/     # dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ sources/               # Source definitions
â”‚   â”‚   â”œâ”€â”€ bronze/                # Raw staging models (8 tables)
â”‚   â”‚   â”œâ”€â”€ silver/                # Cleaned & conformed models
â”‚   â”‚   â””â”€â”€ gold/                  # Business-ready models
â”‚   â”‚       â”œâ”€â”€ gold_fact_order_summary.sql
â”‚   â”‚       â”œâ”€â”€ gold_obt_orders.sql
â”‚   â”‚       â””â”€â”€ fs_customer_features.sql
â”‚   â”œâ”€â”€ macros/                    # Custom dbt macros
â”‚   â”‚   â”œâ”€â”€ business_logic.sql
â”‚   â”‚   â”œâ”€â”€ data_quality.sql
â”‚   â”‚   â””â”€â”€ generate_surrogate_key.sql
â”‚   â”œâ”€â”€ tests/                     # Data quality tests
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ dashboard/                     # Streamlit analytics dashboard
â”‚   â”œâ”€â”€ app.py                     # Main dashboard app
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ overview.py            # KPIs & trends
â”‚   â”‚   â”œâ”€â”€ sales_analytics.py    # Revenue analysis
â”‚   â”‚   â”œâ”€â”€ customer_analytics.py # Customer segmentation
â”‚   â”‚   â”œâ”€â”€ product_analytics.py  # Product performance
â”‚   â”‚   â”œâ”€â”€ delivery_performance.py # Logistics metrics
â”‚   â”‚   â”œâ”€â”€ payment_analytics.py  # Payment insights
â”‚   â”‚   â””â”€â”€ review_analytics.py   # Customer feedback
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ snowflake_connector.py # DB connection
â”‚   â”‚   â””â”€â”€ data_loader.py        # SQL queries
â”‚   â”œâ”€â”€ .streamlit/
â”‚   â”‚   â””â”€â”€ config.toml           # Snowflake theme
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ml_pipeline/                   # Machine learning pipeline
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ 01_eda.ipynb          # Exploratory analysis
â”‚   â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”‚   â””â”€â”€ 03_model_training.ipynb
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ load_training_data.py
â”‚   â”‚   â”œâ”€â”€ train_model.py
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â”œâ”€â”€ models/                    # Trained model artifacts
â”‚   â””â”€â”€ .env                       # Environment variables
â”‚
â””â”€â”€ logs/                          # Execution logs
```

## ğŸ› ï¸ Technology Stack

### Data Infrastructure
- **Cloud Storage**: AWS S3
- **Data Warehouse**: Snowflake
- **Transformation**: dbt Core 1.7.x
- **Orchestration**: Manual / Airflow (optional)

### Analytics & ML
- **Dashboard**: Streamlit 1.31.0
- **Visualization**: Plotly 5.18.0
- **ML Framework**: CatBoost, scikit-learn
- **Data Processing**: pandas 2.1.4, NumPy

### Development
- **Language**: Python 3.11.3
- **Package Manager**: Poetry / pip
- **Version Control**: Git / GitHub
- **IDE**: VS Code

## âœ¨ Features

### Data Pipeline
- âœ… **Automated ELT**: Raw data â†’ Bronze â†’ Silver â†’ Gold
- âœ… **Incremental Loading**: Efficient data processing
- âœ… **Data Quality Checks**: 15+ dbt tests for data integrity
- âœ… **Documentation**: Auto-generated data lineage & docs

### Analytics Dashboard
- ğŸ“Š **Overview Page**: High-level KPIs and trends
- ğŸ’° **Sales Analytics**: Revenue trends, order analysis
- ğŸ‘¥ **Customer Analytics**: LTV, segmentation, behavior
- ğŸ“¦ **Product Analytics**: Category performance, pricing
- ğŸšš **Delivery Performance**: State-level logistics metrics
- ğŸ’³ **Payment Analytics**: Payment methods, installments
- â­ **Review Analytics**: Sentiment analysis, ratings

### Machine Learning
- ğŸ¤– **Customer Segmentation**: Automatic customer clustering
- ğŸ“ˆ **Churn Prediction**: Customer retention forecasting
- ğŸ¯ **Recommendation Engine**: Product recommendations
- ğŸ“Š **Feature Store**: Engineered features in Snowflake

## ï¿½ Data Source

### Brazilian E-Commerce Public Dataset by Olist

**Dataset URL**: [https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

**About the Dataset**:
This is a Brazilian e-commerce public dataset of orders made at Olist Store. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allow viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers.

**Dataset Contents**:
- ğŸ“Š **99,441 orders** with complete information
- ğŸ‘¥ **99,441 unique customers**
- ğŸª **3,095 sellers**
- ğŸ“¦ **32,951 products** across 73 categories
- ğŸŒ **4,119 cities** across 27 Brazilian states
- â­ **99,224 customer reviews** with ratings and comments
- ğŸ’³ **103,886 payment records** (multiple payments per order)

**Data Tables**:
1. `olist_orders_dataset.csv` - Order information
2. `olist_customers_dataset.csv` - Customer details
3. `olist_order_items_dataset.csv` - Items purchased in each order
4. `olist_order_payments_dataset.csv` - Payment methods and values
5. `olist_order_reviews_dataset.csv` - Customer reviews and ratings
6. `olist_products_dataset.csv` - Product catalog
7. `olist_sellers_dataset.csv` - Seller information
8. `olist_geolocation_dataset.csv` - Brazilian zip codes with lat/lng
9. `product_category_name_translation.csv` - Portuguese to English translation

**License**: Public Domain (CC0: Public Domain)

## ï¿½ğŸš€ Setup & Installation

### Prerequisites
```bash
# Required software
- Python 3.11+
- dbt Core 1.7+
- Snowflake account
- AWS account (for S3)
```

### 1. Clone Repository
```bash
git clone https://github.com/hakkache/Data_Engineering_Project_AWS_DBT_SNOWFLAKE.git
cd dbt_snowflake
```

### 2. Install dbt Dependencies
```bash
cd aws_dbt_snowflake_project
pip install dbt-core dbt-snowflake
dbt deps
```

### 3. Configure Snowflake Credentials
```bash
# Edit profiles.yml
nano profiles.yml

# Or use environment variables
export SNOWFLAKE_ACCOUNT=your_account
export SNOWFLAKE_USER=your_user
export SNOWFLAKE_PASSWORD=your_password
export SNOWFLAKE_DATABASE=BRAZILIANECOMMERCE
export SNOWFLAKE_WAREHOUSE=your_warehouse
export SNOWFLAKE_SCHEMA=gold
```

### 4. Setup ML Pipeline
```bash
cd ml_pipeline
cp example.env .env
# Edit .env with your credentials
pip install -r requirements.txt
```

### 5. Install Dashboard Dependencies
```bash
cd dashboard
pip install -r requirements.txt
```

## ğŸ’» Usage

### Run dbt Pipeline
```bash
cd aws_dbt_snowflake_project

# Run all models
dbt run

# Run specific layer
dbt run --select bronze.*
dbt run --select silver.*
dbt run --select gold.*

# Run tests
dbt test

# Generate documentation
dbt docs generate
dbt docs serve
```

### Launch Dashboard
```bash
cd dashboard
streamlit run app.py

# Dashboard will open at http://localhost:8501
```

### Train ML Models
```bash
cd ml_pipeline

# Load training data
python src/load_training_data.py

# Train model
python src/train_model.py

# Generate predictions
python src/predict.py
```

## ğŸ“Š Data Models

### Bronze Layer (Staging)
Raw data loaded from S3 with minimal transformations:
- `bronze_olist_orders` - Order transactions
- `bronze_olist_customers` - Customer information
- `bronze_olist_products` - Product catalog
- `bronze_olist_order_items` - Line items
- `bronze_olist_order_reviews` - Customer reviews
- `bronze_olist_order_payments` - Payment details
- `bronze_olist_sellers` - Seller information
- `bronze_olist_geolocation` - Geographic data

### Silver Layer (Cleaned & Conformed)
Cleaned, deduplicated, and standardized data:
- `silver_orders_enriched` - Orders with customer & product joins
- `silver_reviews_cleaned` - Cleaned review text & sentiment
- `silver_payments_agg` - Aggregated payment data
- `silver_sellers_enhanced` - Enhanced seller metrics
- `silver_fact_orders` - Core fact table

### Gold Layer (Business-Ready)
Aggregated, denormalized tables for analytics:
- `gold_fact_order_summary` - Order-level KPI metrics
- `gold_obt_orders` - One Big Table with 85+ columns
- `fs_customer_features` - Customer feature store for ML

## ğŸ“ˆ Dashboard

### Key Metrics Displayed
- **Total Orders**: 96,096
- **Total Revenue**: R$ 13.59M
- **Total Customers**: 96,096
- **Average Order Value**: R$ 141.45
- **Average Delivery Time**: 12.5 days
- **Customer Satisfaction**: 4.09/5.0

### Dashboard Pages
1. **Overview** - Business KPIs, trends, top segments
2. **Sales Analytics** - Revenue over time, order patterns
3. **Customer Analytics** - LTV distribution, segmentation
4. **Product Analytics** - Category performance, pricing
5. **Delivery Performance** - State-level logistics
6. **Payment Analytics** - Payment method distribution
7. **Review Analytics** - Sentiment analysis, ratings

### Access Dashboard
```bash
cd dashboard
streamlit run app.py
```
Navigate to: `http://localhost:8501`

## ğŸ¤– Machine Learning Pipeline

### Features Engineered
- Customer lifetime value (CLV)
- Recency, Frequency, Monetary (RFM) scores
- Order count & average order value
- Review sentiment scores
- Geographic features
- Product preferences

### Models Trained
- **CatBoost Classifier**: Customer churn prediction
- **Clustering**: Customer segmentation (K-means)
- **Time Series**: Sales forecasting (Prophet)

### Model Performance
- **Accuracy**: 85%+
- **Precision**: 82%
- **Recall**: 79%
- **F1-Score**: 80%

## ğŸ“ Data Quality

### dbt Tests Implemented
- âœ… Uniqueness checks on primary keys
- âœ… Not null constraints on critical fields
- âœ… Referential integrity between tables
- âœ… Value range validations
- âœ… Custom business logic tests

### Run Tests
```bash
dbt test                    # Run all tests
dbt test --select bronze.*  # Test bronze layer
dbt test --select gold.*    # Test gold layer
```

## ğŸ”’ Security

- âœ… Credentials stored in `.env` files (git-ignored)
- âœ… Snowflake role-based access control
- âœ… AWS IAM policies for S3 access
- âœ… No hardcoded secrets in code

## ğŸ“Š Performance Optimization

- âœ… **Incremental models** for large fact tables
- âœ… **Clustering keys** on Snowflake tables
- âœ… **Query caching** in Streamlit (5-min TTL)
- âœ… **Materialized views** for frequently accessed data

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¤ Author

**Hakkache**
- GitHub: [@hakkache](https://github.com/hakkache)
- Project: [Data_Engineering_Project_AWS_DBT_SNOWFLAKE](https://github.com/hakkache/Data_Engineering_Project_AWS_DBT_SNOWFLAKE)

## ğŸ™ Acknowledgments

- **Olist** for providing the [Brazilian E-Commerce Public Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce) on Kaggle
- **dbt Labs** for the amazing transformation framework
- **Snowflake** for the powerful cloud data warehouse platform
- **Streamlit** for the intuitive dashboard framework
- **AWS** for reliable cloud storage infrastructure

## ğŸ“ Support

For questions or issues, please open an issue on GitHub or contact the project maintainer.

---

**Built with â¤ï¸ using dbt, Snowflake, AWS, and Python**
