{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20f6595",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Feature Engineering Complete! ✅\n",
    "\n",
    "**Processed Features:**\n",
    "- Missing values handled (median imputation)\n",
    "- Derived features created (engagement score, efficiency metrics)\n",
    "- Features scaled (StandardScaler)\n",
    "- Top features selected (Mutual Information)\n",
    "\n",
    "**Datasets Ready:**\n",
    "- ✅ Delivery Delay Prediction\n",
    "- ✅ Order Cancellation Prediction\n",
    "- ✅ Customer Satisfaction Prediction\n",
    "\n",
    "**Next Steps:**\n",
    "➡️ **Notebook 03**: Train ML models using the processed datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f2e42",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccee4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path().absolute().parent / 'src'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import joblib\n",
    "\n",
    "# Import custom loader\n",
    "from load_training_data import SnowflakeDataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e93f2",
   "metadata": {},
   "source": [
    "## 2. Load Data from Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d281357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.6.0, Python Version: 3.11.3, Platform: Windows-10-10.0.26100-SP0\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "INFO:load_training_data:Connected to Snowflake successfully\n",
      "INFO:load_training_data:Executing query:\n",
      "\n",
      "        SELECT * FROM gold_obt_orders_ml_export\n",
      "        WHERE 1=1\n",
      "        \n",
      "  LIMIT 50000\n",
      "INFO:snowflake.connector.cursor:query: [SELECT * FROM gold_obt_orders_ml_export WHERE 1=1  LIMIT 50000]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.cursor:Number of results in first chunk: 0\n",
      "INFO:load_training_data:Loaded 50,000 rows with 55 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 rows\n",
      "Shape: (50000, 55)\n"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "loader = SnowflakeDataLoader()\n",
    "\n",
    "# Load OBT data (using sample_size instead of limit)\n",
    "df = loader.load_obt_data(sample_size=50000)\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988ca2e",
   "metadata": {},
   "source": [
    "## 3. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d70f7d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "STDDEV_ITEM_PRICE       89.856\n",
      "REVIEW_SCORE             0.700\n",
      "TARGET_REVIEW_SCORE      0.700\n",
      "ACTUAL_DELIVERY_DAYS     0.008\n",
      "TARGET_DELIVERY_DAYS     0.008\n",
      "dtype: float64\n",
      "\n",
      "Dropping columns: ['STDDEV_ITEM_PRICE']\n"
     ]
    }
   ],
   "source": [
    "# Calculate missing percentage\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_pct[missing_pct > 0].sort_values(ascending=False))\n",
    "\n",
    "# Drop columns with >50% missing values\n",
    "cols_to_drop = missing_pct[missing_pct > 50].index.tolist()\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nDropping columns: {cols_to_drop}\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Fill remaining missing values with median for numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404ecc5",
   "metadata": {},
   "source": [
    "## 4. Create Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99ed8235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns:\n",
      "['ORDER_ID', 'CUSTOMER_ORDER_COUNT', 'CUSTOMER_LIFETIME_VALUE', 'CUSTOMER_AVG_ORDER_VALUE', 'CUSTOMER_TENURE_DAYS', 'DAYS_SINCE_LAST_ORDER', 'ACTUAL_DELIVERY_DAYS', 'ESTIMATED_DELIVERY_DAYS', 'ORDER_YEAR', 'ORDER_QUARTER', 'ORDER_MONTH', 'ORDER_WEEK', 'ORDER_DAY', 'ORDER_DAY_OF_WEEK', 'ORDER_HOUR', 'ORDER_ON_WEEKEND', 'PRODUCT_WEIGHT_G', 'PRODUCT_LENGTH_CM', 'PRODUCT_HEIGHT_CM', 'PRODUCT_WIDTH_CM', 'PRODUCT_VOLUME_CM3', 'PRODUCT_PHOTOS_QTY', 'PRODUCT_ORDER_COUNT', 'PRODUCT_AVG_PRICE', 'SELLER_ORDER_COUNT', 'SELLER_AVG_ITEM_PRICE', 'IS_SAME_STATE', 'IS_SAME_CITY', 'TOTAL_UNIQUE_PRODUCTS', 'TOTAL_ITEMS', 'TOTAL_PRODUCT_VALUE', 'TOTAL_FREIGHT_VALUE', 'TOTAL_ORDER_VALUE', 'AVG_ITEM_PRICE', 'MIN_ITEM_PRICE', 'MAX_ITEM_PRICE', 'MAX_INSTALLMENTS', 'AVG_INSTALLMENTS', 'PAYMENT_TYPES_COUNT', 'PAYMENT_CREDIT_CARD', 'PAYMENT_BOLETO', 'PAYMENT_VOUCHER', 'PAYMENT_DEBIT_CARD', 'REVIEW_SCORE', 'IS_POSITIVE_REVIEW', 'IS_NEGATIVE_REVIEW', 'FREIGHT_TO_PRODUCT_RATIO', 'AVG_VALUE_PER_ITEM', 'TOTAL_CREDIT_EXTENDED', 'IS_DELAYED', 'IS_CANCELED', 'IS_SATISFIED', 'TARGET_REVIEW_SCORE', 'TARGET_DELIVERY_DAYS']\n",
      "\n",
      "============================================================\n",
      "⚠️ SKIPPING DERIVED FEATURES TO PREVENT DATA LEAKAGE\n",
      "============================================================\n",
      "\n",
      "Derived features DISABLED:\n",
      "- customer_engagement_score (uses review_score - LEAKS satisfaction)\n",
      "- delivery_efficiency (uses actual delivery days - LEAKS delay)\n",
      "- price_per_item (OK but not needed)\n",
      "- freight_value_ratio (OK but not needed)\n",
      "\n",
      "Reason: These features use information available AFTER the order outcome,\n",
      "making them perfect predictors (data leakage). We need features that exist\n",
      "at ORDER TIME only.\n",
      "\n",
      "\n",
      "✅ Using only pre-existing features to avoid leakage\n",
      "Current shape: (50000, 54)\n"
     ]
    }
   ],
   "source": [
    "# First, let's see what columns we have\n",
    "print(\"Available columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"⚠️ SKIPPING DERIVED FEATURES TO PREVENT DATA LEAKAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# IMPORTANT: Derived features are DISABLED to prevent data leakage\n",
    "# Features like review scores, delivery times, etc. can leak target information\n",
    "# We'll only use basic numerical features that exist at order time\n",
    "\n",
    "print(\"\"\"\n",
    "Derived features DISABLED:\n",
    "- customer_engagement_score (uses review_score - LEAKS satisfaction)\n",
    "- delivery_efficiency (uses actual delivery days - LEAKS delay)\n",
    "- price_per_item (OK but not needed)\n",
    "- freight_value_ratio (OK but not needed)\n",
    "\n",
    "Reason: These features use information available AFTER the order outcome,\n",
    "making them perfect predictors (data leakage). We need features that exist\n",
    "at ORDER TIME only.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n✅ Using only pre-existing features to avoid leakage\")\n",
    "print(f\"Current shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dde09a",
   "metadata": {},
   "source": [
    "## 5. Prepare Datasets for 3 Prediction Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3eca25a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 42\n",
      "\n",
      "⚠️ Excluded 11 potential leakage features:\n",
      "  - ACTUAL_DELIVERY_DAYS\n",
      "  - IS_SAME_STATE\n",
      "  - IS_SAME_CITY\n",
      "  - REVIEW_SCORE\n",
      "  - IS_POSITIVE_REVIEW\n",
      "  - IS_NEGATIVE_REVIEW\n",
      "  - IS_DELAYED\n",
      "  - IS_CANCELED\n",
      "  - IS_SATISFIED\n",
      "  - TARGET_REVIEW_SCORE\n",
      "\n",
      "✅ Final features: ['CUSTOMER_ORDER_COUNT', 'CUSTOMER_LIFETIME_VALUE', 'CUSTOMER_AVG_ORDER_VALUE', 'CUSTOMER_TENURE_DAYS', 'DAYS_SINCE_LAST_ORDER', 'ESTIMATED_DELIVERY_DAYS', 'ORDER_YEAR', 'ORDER_QUARTER', 'ORDER_MONTH', 'ORDER_WEEK']...\n"
     ]
    }
   ],
   "source": [
    "# Define target columns and data leakage keywords\n",
    "target_cols = ['is_delayed', 'is_canceled', 'is_satisfied']\n",
    "\n",
    "# AGGRESSIVE keywords that indicate potential data leakage\n",
    "# These features use information that's only available AFTER the order outcome\n",
    "leakage_keywords = [\n",
    "    'status', 'delay', 'cancel', 'delivered', 'review', 'score',\n",
    "    'satisfaction', 'satisfied', 'target_', 'is_', 'actual',\n",
    "    'rating', 'comment', 'feedback', 'diff', 'efficiency',\n",
    "    'engagement', 'performance', 'quality'\n",
    "]\n",
    "\n",
    "# Drop non-feature columns\n",
    "drop_cols = [\n",
    "    'order_id', 'customer_id', 'seller_id', 'product_id',\n",
    "    'order_purchase_timestamp', 'order_approved_at',\n",
    "    'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "    'order_estimated_delivery_date', 'shipping_limit_date',\n",
    "    'review_creation_date', 'review_answer_timestamp',\n",
    "    'review_comment_title', 'review_comment_message',\n",
    "    'customer_unique_id', 'customer_zip_code_prefix',\n",
    "    'customer_city', 'customer_state',\n",
    "    'seller_zip_code_prefix', 'seller_city', 'seller_state',\n",
    "    'product_category_name', 'product_category_name_english',\n",
    "    'payment_type', 'order_status'\n",
    "]\n",
    "\n",
    "# Get feature columns (keep only numerical, exclude targets and leakage)\n",
    "feature_cols = []\n",
    "excluded_leakage = []\n",
    "\n",
    "for col in df.columns:\n",
    "    # Skip if not numerical\n",
    "    if df[col].dtype not in [np.int64, np.float64]:\n",
    "        continue\n",
    "    \n",
    "    # Skip if in drop list or target list\n",
    "    if col in drop_cols or col in target_cols:\n",
    "        continue\n",
    "    \n",
    "    # Check for data leakage keywords\n",
    "    col_lower = col.lower()\n",
    "    has_leakage = any(keyword in col_lower for keyword in leakage_keywords)\n",
    "    \n",
    "    if has_leakage:\n",
    "        excluded_leakage.append(col)\n",
    "    else:\n",
    "        feature_cols.append(col)\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "if excluded_leakage:\n",
    "    print(f\"\\n⚠️ Excluded {len(excluded_leakage)} potential leakage features:\")\n",
    "    for col in excluded_leakage[:10]:\n",
    "        print(f\"  - {col}\")\n",
    "print(f\"\\n✅ Final features: {feature_cols[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553e48e",
   "metadata": {},
   "source": [
    "## 6. Split Data into Train/Validation/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d34c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for target columns...\n",
      "\n",
      "⚠️ Expected targets not found!\n",
      "Available target-like columns: ['IS_DELAYED', 'IS_CANCELED', 'IS_SATISFIED', 'TARGET_REVIEW_SCORE', 'TARGET_DELIVERY_DAYS']\n",
      "\n",
      "✅ Using target columns: ['IS_DELAYED', 'IS_CANCELED', 'IS_SATISFIED']\n",
      "\n",
      "============================================================\n",
      "Preparing dataset for: IS_DELAYED\n",
      "============================================================\n",
      "  Samples: 50,000\n",
      "  Class distribution: {0: 45997, 1: 4003}\n",
      "  Train: 30,000, Val: 10,000, Test: 10,000\n",
      "\n",
      "============================================================\n",
      "Preparing dataset for: IS_CANCELED\n",
      "============================================================\n",
      "  Samples: 50,000\n",
      "  Class distribution: {0: 50000}\n",
      "  Train: 30,000, Val: 10,000, Test: 10,000\n",
      "\n",
      "============================================================\n",
      "Preparing dataset for: IS_SATISFIED\n",
      "============================================================\n",
      "  Samples: 50,000\n",
      "  Class distribution: {1: 38658, 0: 11342}\n",
      "  Train: 30,000, Val: 10,000, Test: 10,000\n",
      "\n",
      "✅ Created 3 datasets successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create data directory if not exists\n",
    "data_dir = Path().absolute().parent / 'data'\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# First, check which target columns actually exist\n",
    "print(\"Checking for target columns...\")\n",
    "expected_targets = ['is_delayed', 'is_canceled', 'is_satisfied']\n",
    "existing_targets = [t for t in expected_targets if t in df.columns]\n",
    "\n",
    "if not existing_targets:\n",
    "    # Try to find any target-like columns\n",
    "    target_candidates = [col for col in df.columns \n",
    "                        if any(keyword in col.lower() \n",
    "                              for keyword in ['delay', 'cancel', 'satisf', 'target_'])]\n",
    "    print(f\"\\n⚠️ Expected targets not found!\")\n",
    "    print(f\"Available target-like columns: {target_candidates}\")\n",
    "    existing_targets = target_candidates[:3] if target_candidates else []\n",
    "\n",
    "print(f\"\\n✅ Using target columns: {existing_targets}\")\n",
    "\n",
    "if not existing_targets:\n",
    "    print(\"\\n❌ No target columns found! Cannot proceed with dataset preparation.\")\n",
    "    print(\"Please check your gold_obt_orders_ml_export table schema.\")\n",
    "else:\n",
    "    # Prepare datasets for each task\n",
    "    datasets = {}\n",
    "    \n",
    "    for target in existing_targets:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Preparing dataset for: {target}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get features and target\n",
    "        X = df[feature_cols].copy()\n",
    "        y = df[target].copy()\n",
    "        \n",
    "        # Remove rows with missing target\n",
    "        mask = y.notna()\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        print(f\"  Samples: {len(X):,}\")\n",
    "        print(f\"  Class distribution: {y.value_counts().to_dict()}\")\n",
    "        \n",
    "        # Split: 60% train, 20% validation, 20% test\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        print(f\"  Train: {len(X_train):,}, Val: {len(X_val):,}, Test: {len(X_test):,}\")\n",
    "        \n",
    "        datasets[target] = {\n",
    "            'X_train': X_train, 'y_train': y_train,\n",
    "            'X_val': X_val, 'y_val': y_val,\n",
    "            'X_test': X_test, 'y_test': y_test\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n✅ Created {len(datasets)} datasets successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b73b23",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce192a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling features for targets: ['IS_DELAYED', 'IS_CANCELED', 'IS_SATISFIED']\n",
      "\n",
      "✅ Feature scaling completed and scaler saved\n"
     ]
    }
   ],
   "source": [
    "# Check if we have datasets to scale\n",
    "if not datasets:\n",
    "    print(\"❌ No datasets available for scaling. Run the previous cell first.\")\n",
    "else:\n",
    "    # Get the actual target columns that were created\n",
    "    actual_targets = list(datasets.keys())\n",
    "    print(f\"Scaling features for targets: {actual_targets}\")\n",
    "    \n",
    "    # Fit scaler on the first target's training data\n",
    "    scaler = StandardScaler()\n",
    "    first_target = actual_targets[0]\n",
    "    scaler.fit(datasets[first_target]['X_train'])\n",
    "    \n",
    "    # Apply scaling to all datasets\n",
    "    for target in actual_targets:\n",
    "        for split in ['X_train', 'X_val', 'X_test']:\n",
    "            datasets[target][split] = pd.DataFrame(\n",
    "                scaler.transform(datasets[target][split]),\n",
    "                columns=datasets[target][split].columns,\n",
    "                index=datasets[target][split].index\n",
    "            )\n",
    "    \n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, data_dir / 'scaler.pkl')\n",
    "    print(\"\\n✅ Feature scaling completed and scaler saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca497550",
   "metadata": {},
   "source": [
    "## 8. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83f66ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting top 30 features from 42 total features\n",
      "\n",
      "Selected 30 features:\n",
      "['CUSTOMER_LIFETIME_VALUE', 'CUSTOMER_AVG_ORDER_VALUE', 'ESTIMATED_DELIVERY_DAYS', 'ORDER_YEAR', 'ORDER_QUARTER', 'ORDER_MONTH', 'ORDER_WEEK', 'ORDER_DAY_OF_WEEK', 'PRODUCT_LENGTH_CM', 'PRODUCT_HEIGHT_CM', 'PRODUCT_VOLUME_CM3', 'PRODUCT_PHOTOS_QTY', 'PRODUCT_ORDER_COUNT', 'PRODUCT_AVG_PRICE', 'SELLER_ORDER_COUNT', 'SELLER_AVG_ITEM_PRICE', 'TOTAL_PRODUCT_VALUE', 'TOTAL_FREIGHT_VALUE', 'TOTAL_ORDER_VALUE', 'AVG_ITEM_PRICE', 'MIN_ITEM_PRICE', 'MAX_ITEM_PRICE', 'MAX_INSTALLMENTS', 'AVG_INSTALLMENTS', 'PAYMENT_CREDIT_CARD', 'PAYMENT_BOLETO', 'PAYMENT_VOUCHER', 'FREIGHT_TO_PRODUCT_RATIO', 'AVG_VALUE_PER_ITEM', 'TOTAL_CREDIT_EXTENDED']\n",
      "\n",
      "✅ Feature selection completed and selector saved\n"
     ]
    }
   ],
   "source": [
    "# Check if we have datasets for feature selection\n",
    "if not datasets:\n",
    "    print(\"❌ No datasets available for feature selection. Run the previous cells first.\")\n",
    "else:\n",
    "    # Get the actual target columns\n",
    "    actual_targets = list(datasets.keys())\n",
    "    first_target = actual_targets[0]\n",
    "    \n",
    "    # Get number of features\n",
    "    n_features = datasets[first_target]['X_train'].shape[1]\n",
    "    k_best = min(30, n_features)  # Select top 30 or all if less than 30\n",
    "    \n",
    "    print(f\"Selecting top {k_best} features from {n_features} total features\")\n",
    "    \n",
    "    # Select top K features based on mutual information\n",
    "    selector = SelectKBest(mutual_info_classif, k=k_best)\n",
    "    \n",
    "    # Fit selector on first target's training data\n",
    "    selector.fit(\n",
    "        datasets[first_target]['X_train'],\n",
    "        datasets[first_target]['y_train']\n",
    "    )\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = datasets[first_target]['X_train'].columns[selector.get_support()].tolist()\n",
    "    print(f\"\\nSelected {len(selected_features)} features:\")\n",
    "    print(selected_features)\n",
    "    \n",
    "    # Apply feature selection to all datasets\n",
    "    for target in actual_targets:\n",
    "        for split in ['X_train', 'X_val', 'X_test']:\n",
    "            datasets[target][split] = datasets[target][split][selected_features]\n",
    "    \n",
    "    # Save selector\n",
    "    joblib.dump(selector, data_dir / 'feature_selector.pkl')\n",
    "    print(\"\\n✅ Feature selection completed and selector saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ec984",
   "metadata": {},
   "source": [
    "## 9. Save Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51a109d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n",
      "  ✅ Saved: IS_DELAYED_X_train.parquet\n",
      "  ✅ Saved: IS_DELAYED_y_train.parquet\n",
      "  ✅ Saved: IS_DELAYED_X_val.parquet\n",
      "  ✅ Saved: IS_DELAYED_y_val.parquet\n",
      "  ✅ Saved: IS_DELAYED_X_test.parquet\n",
      "  ✅ Saved: IS_DELAYED_y_test.parquet\n",
      "  ✅ Saved: IS_CANCELED_X_train.parquet\n",
      "  ✅ Saved: IS_CANCELED_y_train.parquet\n",
      "  ✅ Saved: IS_CANCELED_X_val.parquet\n",
      "  ✅ Saved: IS_CANCELED_y_val.parquet\n",
      "  ✅ Saved: IS_CANCELED_X_test.parquet\n",
      "  ✅ Saved: IS_CANCELED_y_test.parquet\n",
      "  ✅ Saved: IS_SATISFIED_X_train.parquet\n",
      "  ✅ Saved: IS_SATISFIED_y_train.parquet\n",
      "  ✅ Saved: IS_SATISFIED_X_val.parquet\n",
      "  ✅ Saved: IS_SATISFIED_y_val.parquet\n",
      "  ✅ Saved: IS_SATISFIED_X_test.parquet\n",
      "  ✅ Saved: IS_SATISFIED_y_test.parquet\n",
      "\n",
      "✅ Feature engineering completed!\n",
      "Processed data saved to: c:\\Users\\hakka\\Documents\\AWS_SNOWFLAKE_DBT_project\\dbt_snowflake\\ml_pipeline\\data\n",
      "Total files created: 18\n"
     ]
    }
   ],
   "source": [
    "# Check if we have datasets to save\n",
    "if not datasets:\n",
    "    print(\"❌ No datasets available to save. Run the previous cells first.\")\n",
    "else:\n",
    "    # Save all datasets using actual target names\n",
    "    print(\"Saving processed datasets...\")\n",
    "    \n",
    "    for target in datasets.keys():\n",
    "        task_name = target.replace('is_', '')\n",
    "        \n",
    "        for split_name, split_data in datasets[target].items():\n",
    "            filename = f\"{task_name}_{split_name}.parquet\"\n",
    "            \n",
    "            # Convert Series to DataFrame if needed (for y_train, y_test, y_val)\n",
    "            if isinstance(split_data, pd.Series):\n",
    "                split_data = split_data.to_frame()\n",
    "            \n",
    "            split_data.to_parquet(data_dir / filename)\n",
    "            print(f\"  ✅ Saved: {filename}\")\n",
    "    \n",
    "    print(f\"\\n✅ Feature engineering completed!\")\n",
    "    print(f\"Processed data saved to: {data_dir}\")\n",
    "    print(f\"Total files created: {len(list(data_dir.glob('*.parquet')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3fc4a9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Created Artifacts:**\n",
    "- 18 parquet files (3 tasks × 6 files per task)\n",
    "- `scaler.pkl` - StandardScaler fitted on training data\n",
    "- `feature_selector.pkl` - SelectKBest with top 30 features\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run `03_model_training.ipynb` to train ML models\n",
    "2. Evaluate model performance\n",
    "3. Deploy best models to production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
